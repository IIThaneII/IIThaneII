{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduction to the BeautifulSoup library.\n",
    "from bs4 import BeautifulSoup\n",
    "# import lxml as subsequences if the html parser is not work.\n",
    "\n",
    "with open(\"/Users/ez/OneDrive/M치y t칤nh/codeprojects/PYTHON/100_days_of_Py/10_day(45-50)/b-soup/website.html\", encoding=\"utf8\") as file:\n",
    "    contents = file.read()                                                                                    # Fix the UnicodeDecodeError: 'charmap' codec can't decode byte 0x9d in position 281: character maps to <undefined> bug.\n",
    "\n",
    "soup = BeautifulSoup(contents, \"html.parser\") # lxml\n",
    "\n",
    "# print(soup.title)   \n",
    "# print(soup.title.name) # Name of the tag which is 'title'.\n",
    "# print(soup.title.string) # Content of title tag.\n",
    "print(soup.prettify()) # Indented format all the content of the html file.\n",
    "print(soup.p) # Or <a>, <li>,... tag.\n",
    "\n",
    "#----------------------------------------------------------#\n",
    "\n",
    "all_anchor_tag = soup.find_all(name=\"a\") # Print out all anchors tag.\n",
    "print(all_anchor_tag)\n",
    "for tag in all_anchor_tag:\n",
    "    # print(tag.getText())\n",
    "    print(tag.get(\"href\")) # Print out the links contained in the anchor tags.\n",
    "\n",
    "#----------------------------------------------------------#\n",
    "\n",
    "heading = soup.find(name=\"h1\", id=\"name\")\n",
    "print(heading)\n",
    "\n",
    "section_heading = soup.find(name=\"h3\", class_=\"heading\") # Cannot use the word class because it is a keyword in python.\n",
    "print(section_heading)\n",
    "\n",
    "company_url = soup.select_one(selector=\"p a\") # To select a specific anchor tag using CSS selector.\n",
    "print(company_url)\n",
    "\n",
    "name = soup.select_one(\"#name\") # ID\n",
    "print(name)\n",
    "\n",
    "headings = soup.select(\".heading\") # Class \n",
    "print(headings)\n",
    "\n",
    "#----------------------------------------------------------#\n",
    "\n",
    "form_tag = soup.find(\"input\")\n",
    "max_length = form_tag.get(\"maxlength\")\n",
    "print(max_length) # Get the maxlength value\n",
    "\n",
    "#----------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 游뚿 Scraping a Live Website\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "response = requests.get(\"https://news.ycombinator.com/news\")\n",
    "yc_we_page = response.text\n",
    "\n",
    "soup = BeautifulSoup(yc_we_page, \"html.parser\")\n",
    "articles = soup.find_all(name=\"a\", class_=\"titlelink\")\n",
    "article_texts = []\n",
    "article_links = []\n",
    "for article in articles:\n",
    "    text = article.getText()\n",
    "    article_texts.append(text)\n",
    "    link = article.get(\"href\")\n",
    "    article_links.append(link)\n",
    "article_upvote = [int(score.getText().split(\" \")[0]) for score in soup.find_all(name=\"span\", class_=\"score\")]\n",
    "\n",
    "max_points_index = article_upvote.index(max(article_upvote))\n",
    "print(article_texts[max_points_index])\n",
    "print(article_links[max_points_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 游뚿 Go to the \"https//.../robots.txt\" to read the ethical codes of conduct before scraping the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 游뚿 100 Movies to watch.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = \"https://web.archive.org/web/20200518073855/https://www.empireonline.com/movies/features/best-movies-2/\"\n",
    "\n",
    "response = requests.get(URL)\n",
    "the_100_mv_page = response.text\n",
    "\n",
    "soup = BeautifulSoup(the_100_mv_page, \"html.parser\")\n",
    "movies_list = soup.find_all(name=\"h3\", class_=\"title\")\n",
    "movies_list_reverse = movies_list.reverse()\n",
    "\n",
    "with open(\"/Users/ez/OneDrive/M치y t칤nh/codeprojects/PYTHON/100_days_of_Py/10_day(45-50)/100-movies/list.txt\", mode=\"w\", encoding=\"utf-8\") as f:\n",
    "    for movie in movies_list:\n",
    "        f.write(f\"{movie.getText()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 游뚿 100 Movies to watch solution.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = \"https://web.archive.org/web/20200518073855/https://www.empireonline.com/movies/features/best-movies-2/\"\n",
    "\n",
    "# Write your code below this line 游녢\n",
    "\n",
    "response = requests.get(URL)\n",
    "website_html = response.text\n",
    "\n",
    "soup = BeautifulSoup(website_html, \"html.parser\")\n",
    "\n",
    "all_movies = soup.find_all(name=\"h3\", class_=\"title\")\n",
    "\n",
    "movie_titles = [movie.getText() for movie in all_movies]\n",
    "movies = movie_titles[::-1]\n",
    "\n",
    "with open(\"movies.txt\", mode=\"w\") as file:\n",
    "    for movie in movies:\n",
    "        file.write(f\"{movie}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 游뚿 Sportify playlist generator of top 100 Billboard Music any day.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import spotipy\n",
    "from spotipy import oauth2\n",
    "\n",
    "SPOTIPY_CLIENT_ID = 'Your key'\n",
    "SPOTIPY_CLIENT_SECRET = 'Your secret key'\n",
    "SPOTIPY_REDIRECT_URI = 'http://example.com'\n",
    "\n",
    "sp_oauth = oauth2.SpotifyOAuth(\n",
    "    SPOTIPY_CLIENT_ID, \n",
    "    SPOTIPY_CLIENT_SECRET,\n",
    "    SPOTIPY_REDIRECT_URI, \n",
    "    scope=\"playlist-modify-private\", \n",
    "    show_dialog=True,\n",
    "    # cache_path=\"/Users/ez/OneDrive/M치y t칤nh/codeprojects/PYTHON/100_days_of_Py/10_day(45-50)/sportify-playlist/.cache\"\n",
    "    )\n",
    "\n",
    "#------------------------ Get the access token ------------------------#\n",
    "\n",
    "# ad = sp_oauth.get_access_token(as_dict=False)\n",
    "# print(ad)\n",
    "\n",
    "#------------------------ Get the access token ------------------------#\n",
    "\n",
    "header = {\n",
    "    \"Authorization\": \"Bearer your_key\"\n",
    "}\n",
    "\n",
    "sp = spotipy.Spotify(auth=header, oauth_manager=sp_oauth)\n",
    "user_id = sp.current_user()[\"id\"]\n",
    "scrap_day = input(\"Which year do you want to travel to? Type the date in this format YYYY-MM-DD: \")\n",
    "\n",
    "#------------------------ get 100 billboard songs of the day ------------------------#\n",
    "\n",
    "URL = f\"https://www.billboard.com/charts/hot-100/{scrap_day}/\"\n",
    "\n",
    "response = requests.get(URL)\n",
    "website_html = response.text\n",
    "\n",
    "soup = BeautifulSoup(website_html, \"html.parser\")\n",
    "\n",
    "song_titles = soup.select(selector=\"li #title-of-a-story\")\n",
    "with open(\"/Users/ez/OneDrive/M치y t칤nh/codeprojects/PYTHON/100_days_of_Py/10_day(45-50)/sportify-playlist/songs.txt\", mode=\"w\", encoding=\"utf-8\") as f:\n",
    "    for song in song_titles:\n",
    "        f.write(f\"{song.getText().strip()}\\n\")\n",
    "\n",
    "#------------------------ add songs from the list_song to the playlist ------------------------#\n",
    "\n",
    "with open(\"/Users/ez/OneDrive/M치y t칤nh/codeprojects/PYTHON/100_days_of_Py/10_day(45-50)/sportify-playlist/songs.txt\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    song_names = f.read().splitlines() # Readlines without endline\n",
    "\n",
    "song_uris = []\n",
    "year = scrap_day.split(\"-\")[0]\n",
    "for song in song_names:\n",
    "    result = sp.search(q=f\"track:{song} year:{year}\", type=\"track\")\n",
    "    print(result)\n",
    "    try:\n",
    "        uri = result[\"tracks\"][\"items\"][0][\"uri\"]\n",
    "        song_uris.append(uri)\n",
    "    except IndexError:\n",
    "        print(f\"{song} doesn't exist in Spotify. Skipped.\")\n",
    "playlist = sp.user_playlist_create(user=user_id, name=f\"{scrap_day} Billboard 100\", public=False)\n",
    "\n",
    "sp.playlist_add_items(playlist_id=playlist[\"id\"], items=song_uris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 游뚿 Automated Amazon price tracker.\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import smtplib\n",
    "\n",
    "my_email = \"nguyennam2741@yahoo.com\"\n",
    "my_password = \"G-5*EqpY!mLrijH\"\n",
    "\n",
    "URL = f\"https://www.amazon.com/Kindle-Paperwhite-Essentials-Bundle-including/dp/B09FBXR5Q8/ref=sr_1_3?crid=1KAVVINZLWW7F&keywords=kindle&qid=1659596710&sprefix=kindl%2Caps%2C713&sr=8-3\"\n",
    "\n",
    "header = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\",\n",
    "    \"Accept-Language\": \"vi-VN,vi;q=0.9,fr-FR;q=0.8,fr;q=0.7,en-US;q=0.6,en;q=0.5\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\"\n",
    "}\n",
    "\n",
    "response = requests.get(URL, headers=header)\n",
    "website_html = response.text\n",
    "\n",
    "soup = BeautifulSoup(website_html, \"html.parser\")#.encode(\"utf-8\")\n",
    "price = soup.find(name=\"span\", class_=\"a-offscreen\").getText().split(\"$\")\n",
    "if float(price[1]) < 150:\n",
    "    with smtplib.SMTP(\"smtp.mail.yahoo.com\") as connection:\n",
    "        connection.starttls()\n",
    "        connection.login(user=my_email, password=my_password)\n",
    "        connection.sendmail(from_addr=my_email, to_addrs=\"ntguppy13@gmail.com\", msg=\"HOT HOT\\n\\nThe price had dropped below your buy price!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selenium_doc_url = \"https://selenium-python.readthedocs.io/locating-elements.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 游뚿 Python.org events list.\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get(\"https://www.python.org/\")\n",
    "\n",
    "events = {}\n",
    "\n",
    "list_length = len(driver.find_elements(By.XPATH, '//*[@id=\"content\"]/div/section/div[2]/div[2]/div/ul/li')) \n",
    "# Can copy the xpath by right click on the element and choose copy then Xpath\n",
    "# Use the get_attribute(\"...\") to get the thing you want.\n",
    "for i in range(1, list_length+1):\n",
    "    time = driver.find_element(By.XPATH, f'//*[@id=\"content\"]/div/section/div[2]/div[2]/div/ul/li[{i}]/time').text\n",
    "    name = driver.find_element(By.XPATH, f'//*[@id=\"content\"]/div/section/div[2]/div[2]/div/ul/li[{i}]/a').text\n",
    "    events[i] = {\n",
    "        \"time\": f\"{time}\",\n",
    "        \"name\": f\"{name}\",\n",
    "    }\n",
    "\n",
    "print(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 游뚿 Wikipedia articles.\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get(\"https://en.wikipedia.org/wiki/Main_Page/\")\n",
    "\n",
    "article_cnt = driver.find_element(By.XPATH, '//*[@id=\"articlecount\"]/a[1]')\n",
    "print(article_cnt.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 游뚿 Form filling exercise\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get(\"http://secure-retreat-92358.herokuapp.com/\")\n",
    "\n",
    "first_name = driver.find_element(By.XPATH, '/html/body/form/input[1]')\n",
    "first_name.send_keys(\"Thanh\")\n",
    "\n",
    "last_name = driver.find_element(By.XPATH, '/html/body/form/input[2]')\n",
    "last_name.send_keys(\"Nguyen\")\n",
    "\n",
    "email = driver.find_element(By.XPATH, '/html/body/form/input[3]')\n",
    "email.send_keys(\"ThanhNguyen@gmail.com\")\n",
    "\n",
    "button = driver.find_element(By.XPATH, '/html/body/form/button')\n",
    "button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 游뚿 Auto CookieClicker play bot.\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get(\"http://orteil.dashnet.org/experiments/cookie/\") \n",
    "\n",
    "continue_game = 1\n",
    "while continue_game == 1:\n",
    "    try:\n",
    "        cookie_clicker = driver.find_element(By.XPATH, '//*[@id=\"cookie\"]')\n",
    "        cookie_clicker.click()\n",
    "\n",
    "        crursor = driver.find_element(By.XPATH, '//*[@id=\"buyCursor\"]/b')\n",
    "        grandma = driver.find_element(By.XPATH, '//*[@id=\"buyGrandma\"]/b')\n",
    "        factory = driver.find_element(By.XPATH, '//*[@id=\"buyFactory\"]/b')\n",
    "        mine = driver.find_element(By.XPATH, '//*[@id=\"buyMine\"]/b')\n",
    "        shipment = driver.find_element(By.XPATH, '//*[@id=\"buyShipment\"]/b')\n",
    "        alchemy_lab = driver.find_element(By.XPATH, '//*[@id=\"buyAlchemy lab\"]/b')\n",
    "        portal = driver.find_element(By.XPATH, '//*[@id=\"buyPortal\"]/b')\n",
    "        time_machine = driver.find_element(By.XPATH, '//*[@id=\"buyTime machine\"]/b')\n",
    "        upgrade_list = [crursor, grandma, factory, mine, shipment, alchemy_lab, portal, time_machine]\n",
    "        money_list = [int(n.text.split(\" - \")[1].replace(\",\",\"\")) for n in upgrade_list]\n",
    "        money = int(driver.find_element(By.XPATH, '//*[@id=\"money\"]').text)\n",
    "\n",
    "        if money > min(money_list):\n",
    "            i = money_list.index(min(money_list))\n",
    "            upgrade_list[i].click()\n",
    "    except StaleElementReferenceException:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 游뚿 LinkedIn auto job save.\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get(\"https://www.linkedin.com/jobs/search/?f_LF=f_AL&geoId=102257491&keywords=python%20developer&location=London%2C%20England%2C%20United%20Kingdom&redirect=false&position=1&pageNum=0\")\n",
    "\n",
    "sign_in = driver.find_element(By.XPATH, '/html/body/div[1]/header/nav/div/a[2]')\n",
    "sign_in.click()\n",
    "email_sign_in = driver.find_element(By.XPATH, '//*[@id=\"username\"]')\n",
    "email_sign_in.send_keys('********************')\n",
    "password_sign_in = driver.find_element(By.XPATH, '//*[@id=\"password\"]')\n",
    "password_sign_in.send_keys('***************')\n",
    "sign_in_button = driver.find_element(By.XPATH, '//*[@id=\"organic-div\"]/form/div[3]/button')\n",
    "sign_in_button.click()\n",
    "job_list = driver.find_elements(By.CSS_SELECTOR, \"a .disabled ember-view job-card-container__link job-card-list__title\")\n",
    "for job in job_list:\n",
    "    try:\n",
    "        time.sleep(2)\n",
    "        job.click()\n",
    "        time.sleep(2)\n",
    "        save_button = driver.find_element(By.CSS_SELECTOR, \"button .jobs-save-button artdeco-button artdeco-button--3 artdeco-button--secondary\")\n",
    "        save_button.click()\n",
    "    except NoSuchElementException:\n",
    "        print(\"No save button, skipped.\")\n",
    "time.sleep(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 游뚿 Auto Tinder swiping bot.\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time \n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get(\"https://tinder.com/\")\n",
    "time.sleep(2)\n",
    "log_in = driver.find_element(By.XPATH, '//*[@id=\"q554704800\"]/div/div[1]/div/main/div[1]/div/div/div/div/header/div/div[2]/div[2]/a')\n",
    "log_in.click()\n",
    "time.sleep(2)\n",
    "more_opt = driver.find_element(By.XPATH, '//*[@id=\"q-1173676276\"]/div/div/div[1]/div/div/div[3]/span/button')\n",
    "more_opt.click()\n",
    "time.sleep(2)\n",
    "log_in_gg = driver.find_element(By.XPATH, '//*[@id=\"q-1173676276\"]/div/div/div[1]/div/div/div[3]/span/div[2]/button/span[2]')\n",
    "log_in_gg.click()\n",
    "base_window = driver.window_handles[0]\n",
    "fb_login_window = driver.window_handles[1]\n",
    "driver.switch_to.window(fb_login_window)\n",
    "time.sleep(2)\n",
    "email_address = driver.find_element(By.XPATH, '//*[@id=\"email\"]')\n",
    "email_address.send_keys(\"Your email\")\n",
    "time.sleep(2)\n",
    "password = driver.find_element(By.XPATH, '//*[@id=\"pass\"]')\n",
    "password.click()\n",
    "password.send_keys(\"Your password\")\n",
    "password.send_keys(Keys.ENTER)\n",
    "driver.switch_to.window(base_window)\n",
    "time.sleep(10)\n",
    "allow_locate = driver.find_element(By.XPATH, '//*[@id=\"q-1173676276\"]/div/div/div/div/div[3]/button[1]/span')\n",
    "allow_locate.click()\n",
    "time.sleep(2)\n",
    "noti_enable = driver.find_element(By.XPATH, '//*[@id=\"q-1173676276\"]/div/div/div/div/div[3]/button[1]/span')\n",
    "noti_enable.click()\n",
    "time.sleep(2)\n",
    "cookie_acpt = driver.find_element(By.XPATH, '//*[@id=\"q554704800\"]/div/div[2]/div/div/div[1]/div[1]/button/span')\n",
    "cookie_acpt.click()\n",
    "time.sleep(10)\n",
    "for i in range(100):\n",
    "    try:\n",
    "        like_button = driver.find_element(By.XPATH, '//*[@id=\"q554704800\"]/div/div[1]/div/main/div[1]/div/div')\n",
    "        like_button.send_keys(Keys.ARROW_LEFT)\n",
    "    except NoSuchElementException:\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d20d4eed201e135a92e4cffa48ff938af52352adb5646d0546e5d7d302dddd31"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
